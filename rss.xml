<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0"
     xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd"
     xmlns:media="http://search.yahoo.com/mrss/">
  <channel>
    <title>NotebookLM语音播客</title>
    <link>https://zhxianlucky.github.io/notebooklm-podcast</link>
    <description>NotebookLM自动生成音频播客</description>
    <language>zh-cn</language>
    <itunes:author>zhxianlucky</itunes:author>
    <itunes:explicit>no</itunes:explicit>
    <itunes:type>episodic</itunes:type>
    
        <item>
          <title>ReflAct: World-Grounded Decision Making in LLM Agents via Goal-State   Reflection</title>
          <enclosure url="https://zhxianlucky.github.io/notebooklm-podcast/output/ReflAct:_World-Grounded_Decision_Making_in_LLM_Agents_via_Goal-State___Reflection.mp3" length="14733356" type="audio/mpeg"/>
          <guid>2505.15182</guid>
          <pubDate>Wed, 25 Jun 2025 01:23:12 -0000</pubDate>
          <description>Recent advances in LLM agents have largely built on reasoning backbones like ReAct, which interleave thought and action in complex environments. However, ReAct often produces ungrounded or incoherent reasoning steps, leading to misalignment between the agent's actual state and goal. Our analysis finds that this stems from ReAct's inability to maintain consistent internal beliefs and goal alignment, causing compounding errors and hallucinations. To address this, we introduce ReflAct, a novel backbone that shifts reasoning from merely planning next actions to continuously reflecting on the agent's state relative to its goal. By explicitly grounding decisions in states and enforcing ongoing goal alignment, ReflAct dramatically improves strategic reliability. This design delivers substantial empirical gains: ReflAct surpasses ReAct by 27.7% on average, achieving a 93.3% success rate in ALFWorld. Notably, ReflAct even outperforms ReAct with added enhancement modules (e.g., Reflexion, WKM), showing that strengthening the core reasoning backbone is key to reliable agent performance.</description>
          <itunes:duration>15:20</itunes:duration>
        </item>
        
        <item>
          <title>Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights</title>
          <enclosure url="https://zhxianlucky.github.io/notebooklm-podcast/output/Drag-and-Drop_LLMs:_Zero-Shot_Prompt-to-Weights.mp3" length="14916524" type="audio/mpeg"/>
          <guid>2506.16406</guid>
          <pubDate>Wed, 25 Jun 2025 01:23:16 -0000</pubDate>
          <description>Modern Parameter-Efficient Fine-Tuning (PEFT) methods such as low-rank adaptation (LoRA) reduce the cost of customizing large language models (LLMs), yet still require a separate optimization run for every downstream dataset. We introduce \textbf{Drag-and-Drop LLMs (\textit{DnD})}, a prompt-conditioned parameter generator that eliminates per-task training by mapping a handful of unlabeled task prompts directly to LoRA weight updates. A lightweight text encoder distills each prompt batch into condition embeddings, which are then transformed by a cascaded hyper-convolutional decoder into the full set of LoRA matrices. Once trained in a diverse collection of prompt-checkpoint pairs, DnD produces task-specific parameters in seconds, yielding i) up to \textbf{12,000$\times$} lower overhead than full fine-tuning, ii) average gains up to \textbf{30\%} in performance over the strongest training LoRAs on unseen common-sense reasoning, math, coding, and multimodal benchmarks, and iii) robust cross-domain generalization despite never seeing the target data or labels. Our results demonstrate that prompt-conditioned parameter generation is a viable alternative to gradient-based adaptation for rapidly specializing LLMs. Our project is available at \href{https://jerryliang24.github.io/DnD}{https://jerryliang24.github.io/DnD}.</description>
          <itunes:duration>15:32</itunes:duration>
        </item>
        
  </channel>
</rss>